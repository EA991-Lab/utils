{"cells":[{"cell_type":"markdown","metadata":{"id":"M7ywkXjnKtk7"},"source":["\n","Scikit-Learn\n","============\n","\n","[Scikit-Learn](http://scikit-learn.org/) é a principal biblioteca para\n","aprendizado de máquina do Python. Sua API é extremamente simples e\n","bem-pensada. Diferente de muitas bibliotecas deste tipo, ela é um\n","exemplo de documentação e qualidade de código. Vamos ver o básico, mas\n","sinta-se encorajado a olhar a documentação e descobrir tudo que está\n","implementado.\n","\n","Pré-processamento: atributos numérico\n","-------------------------------------\n","\n","Conforme vimos em aula, é comum termos que realizar algumas\n","transformações nas bases de dados para que os dados estejam de acordo\n","com as premissas dos algoritmos. Vamos ver algumas transformaçoes usando\n","a API do sklearn. É importante ressaltar que todas as técnicas\n","implementadas na sklearn são organizadas utilizando Orientação à\n","Objetos. No caso das transformações, vamos ver inicialmente três\n","objetos: `sklearn.preprocessing.StandardScaler`,\n","`sklearn.preprocessing.MinMaxScaler`,\n","`sklearn.preprocessing.Normalizer`.\n","\n","Independente da transformação, a API consiste de três métodos:\n","\n","-   `fit`: realiza os ajustes para a função de transformação\n","    considerando a base de dados;\n","-   `transform`: recebe uma base de dados e retorna ela com a\n","    transformação aplicada;\n","-   `fit_transform`: faz os dois passos anteriores na mesma base de\n","    dados.\n","\n","Considerando as três transformações comentadas acima, para\n","`StandardScaler` (que transforma um atributo para média zero e desvio\n","padrão um) é necessário computar o valor da média e desvio padrão,\n","`MinMaxScaler` (que transforma os valores de um atributo em uma faixa,\n","usualmente \\[0,1\\]) é necessário encontrar o menor e maior valor\n","observada, e por fim, no `Normalizer` (que normaliza objetos para terem\n","norma 1: $\\|\\mathbf{x}_i\\|=1$) o passo `fit` não realiza nenhuma\n","operação."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OamdQhFxKtk8"},"outputs":[],"source":["import pandas as pd\n","import sklearn.preprocessing as pp\n","#biblioteca com arrays N-dimensionais e computação científica em geral\n","import numpy as np\n","\n","#criando um dataframe do pandas manualmente\n","X = pd.DataFrame({'A1': [1.,2.,3.,4.,5.], 'A2':[100.,200.,300.,41.,5.]})\n","\n","Xscaled = pp.StandardScaler().fit_transform(X)\n","scaler = pp.StandardScaler()\n","scaler.fit(X)\n","#X.values corresponde ao array numpy que os dados do dataFrame estão usando\n","assert np.all(Xscaled == scaler.transform(X.values))\n","print(\"Médias de cada atributo original\", scaler.mean_)\n","\n","minmax = pp.MinMaxScaler()\n","Xmm = minmax.fit_transform(X)\n","print(\"X normalizado [0,1]:\\n\",Xmm)\n","\n","norm = pp.Normalizer()\n","Xnorm = norm.transform(X)\n","print(\"X com norma 1 por linha:\\n\",Xnorm)\n"]},{"cell_type":"markdown","metadata":{"id":"NxVhtNGXKtlC"},"source":["### Exercício: Verifique que Xscaled tem média zero e desvio padrão um para cada atributo\n","\n","Pré-processamento: atributos categóricos\n","----------------------------------------\n","\n","Como vimos em algumas aulas, nem todos os algoritmos sabem lidar bem com\n","atributos categóricos. Na verdade, essa é uma das áreas deficientes na\n","biblioteca sklearn. Os algoritmos implementados assumem valores\n","contínuos. Para poder usar os algoritmos com atributos categóricos,\n","vamos convertê-los em $V$ atributos binários, sendo $V$ o número de\n","diferentes valores que o atributo pode ter. Essa transformação é\n","denominada `OneHotEncoder`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fdam1g1NKtlD"},"outputs":[],"source":["enc = pp.OneHotEncoder()\n","X = [['estudante', 'computação', 'linux'], ['estudante', 'eiar', 'windows']]\n","enc.fit(X)\n","Xt = enc.transform([['estudante', 'computação', 'windows'],\n","                    ['estudante', 'eiar', 'linux']]).toarray()\n","print(Xt)"]},{"cell_type":"markdown","metadata":{"id":"plhtiargKtlE"},"source":["Pré-processamento: discretização\n","--------------------------------\n","\n","Para discretizar um atributo numérico, podemos utilizar o\n","`KBinsDiscretizer`. Essa classe implementa os dois modos de\n","discretização visto em sala: largura fixa (`strategy=`\\'uniform\\') e\n","frequência fixa (`strategy=`\\'quantile\\')."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"beCw94FOKtlF"},"outputs":[],"source":["X2 = pd.DataFrame({'A1': [32,34,43,45,51,59,62,67,68,69,70,71,72]})\n","larg_fixa = pp.KBinsDiscretizer(n_bins = 8, strategy = 'uniform', encode = 'ordinal')\n","larg_fixa.fit(X2)\n","print(larg_fixa.transform(X2))\n","faixas = [ f\"[{x:.0f},{y:.0f})\" for x,y in zip(larg_fixa.bin_edges_[0], larg_fixa.bin_edges_[0][1:])]\n","print(\"Intervalos:\" + \", \".join(faixas))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F6TNJ8mCKtlJ"},"outputs":[],"source":["X2 = pd.DataFrame({'A1': [32,34,43,45,51,59,62,67,68,69,70,71,72]})\n","freq_fixa = pp.KBinsDiscretizer(n_bins = 5, strategy = 'quantile', encode = 'ordinal')\n","freq_fixa.fit(X2)\n","print(freq_fixa.transform(X2))\n","faixas = [ f\"[{x:.0f},{y:.0f})\" for x,y in zip(freq_fixa.bin_edges_[0], freq_fixa.bin_edges_[0][1:])]\n","print(\"Intervalos:\" + \", \".join(faixas))"]},{"cell_type":"markdown","metadata":{"id":"gBWs5VqFg9Re"},"source":["Scikit-Learn - Parte 2\n","======================\n","\n","Vimos anteriormente como utilizar a biblioteca Scikit-Learn para realizar o pré-processamento dos dados.\n","Veremos agora o básico sobre como rodar algoritmos de classificação e regressão nessa biblioteca.\n","\n","Principais métodos\n","------------------\n","\n","Os algoritmos de aprendizado de máquina implementados na Scikit-Learn utilizam uma API bastante intuitiva, você trabalhará frequentemente com três métodos:\n","- `fit`: treina um modelo, para algoritmos de aprendizado supervisionado você deve passar dois parâmetros `X` (matriz de dados que pode ser um `pandas.DataFrame`) e `y` (vetor com rótulos dos dados, pode ser uma coluna de um `pandas.DataFrame`, também conhecido como `pandas.Series`);\n","- `predict`: realiza a predição para novos dados, recebe como primeiro parâmetro uma matriz de objetos de teste;\n","- `predict_proba`: similar ao `predict`, mas ao invés de retornar apenas os rótulos de classe preditas, retorna a probabilidade _posteriori_ de cada classe, ou seja, $p(c|x)$.\n","\n","Um breve exemplo\n","----------------\n","\n","Vamos ver como treinar o modelo Naïve Bayes considerando todos os atributos distribuídos de acordo com uma distribuição Gaussiana.\n","Este algoritmo está implementado como `GaussianNB` no módulo `sklearn.naive_bayes`.\n","Vamos utilizar a base de dados Iris que tem 150 objetos, 4 atributos e 3 classes.\n"]},{"cell_type":"code","metadata":{"id":"FU6rjSGhg0Sp"},"source":["import pandas as pd\n","from sklearn.naive_bayes import GaussianNB\n","\n","dados = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\", header = None)\n","print(dados.head())\n","#sklearn trabalha com X e y separados\n","X, y = dados.drop(4, axis = 1), dados[4]\n","clf = GaussianNB()\n","clf.fit(X,y)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N6TUdzVXpHCJ"},"source":["Podemos examinar o modelo por meio dos atributos do objeto `clf`. Por exemplo, podemos ver a média estimada para cada atributo em `clf.theta_`. Como temos 3 classes e 4 atributos, este atributo corresponde a uma matriz 3x4."]},{"cell_type":"code","metadata":{"id":"cYeJnXGkpUfm"},"source":["print(clf.theta_)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1N4yLGnSqOxt"},"source":["Você pode ver todas as informações armazenadas no modelo na [documentação dessa classe](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html).\n","Podemos examinar o erro obtido por esse modelo considerando os dados de treinamento por meio do `predict`."]},{"cell_type":"code","metadata":{"id":"D25MnlaMqVTA"},"source":["predicoes = clf.predict(X)\n","qtd_erros = predicoes != y\n","taxa_erro = qtd_erros.sum() / X.shape[0]\n","print(f\"Erro de treinamento: {taxa_erro:.3f}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JYTpXQWHq5J5"},"source":["Esse erro não é muito informativo, seria melhor vermos quanto ele erra em dados que não foram usados no treinamento. Para ajudar nisso, podemos utilizar [StratifiedShuffleSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html#sklearn.model_selection.StratifiedShuffleSplit).\n","Com ele, temos um conjunto de divisões aleatórias da base de dados que mantém a proporção de objetos de cada classe em cada divisão.\n"]},{"cell_type":"code","metadata":{"id":"dd3HSHToxnCd"},"source":["from sklearn.model_selection import StratifiedShuffleSplit\n","divisoes = StratifiedShuffleSplit(n_splits=2, test_size=0.5, random_state=0)\n","for idx_treino, idx_teste in divisoes.split(X, y):\n","    print(\"Objetos no treino:\", idx_treino)\n","    print(\"Objetos no teste:\", idx_teste)\n","    Xtreino, ytreino = X.iloc[idx_treino], y.iloc[idx_treino]\n","    Xteste, yteste = X.iloc[idx_teste], y.iloc[idx_teste]\n","    erros = clf.fit(Xtreino, ytreino).predict(Xteste) != yteste\n","    print(f\"Erro de teste: {erros.sum()/len(idx_teste):.3f}\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tPTXiu0A0ovL"},"source":["É possível notar um pequeno aumento na taxa de erro. Isso é esperado. A taxa de erro no treinamento tende a ser uma estimativa otimista, afinal, aqueles dados foram usados para treinar o modelo.\n","\n","Note que o método `fit` retorna o próprio objeto, portanto, podemos utilizar os métodos por meio de uma [interface fluente](https://en.wikipedia.org/wiki/Fluent_interface).\n","Um conjunto considerável de algoritmos está implementado [na biblioteca](https://scikit-learn.org/stable/supervised_learning.html). O fato deles usarem a mesma API facilita bastante seu uso.\n"]},{"cell_type":"markdown","metadata":{"id":"rSj7Zb4NMzrr"},"source":["Visualizando fronteiras de decisão\n","==================================\n","\n","Como vimos nas aulas, quando temos apenas dois atributos é interessante visualizar a fronteira de decisão entre as classes.\n","Uma forma simples, que funciona para qualquer modelo independente da complexidade de sua fronteira, é realizar a predição para cara ponto em um _grid_ pré-definido.\n","\n","Para ilustrar isso, transformaremos os atributos originais da base Iris (comprimento da sépala, largura da sépala, comprimento da pétala e largura da pétala) em atributos de área e consideramos pontos entre os limites de valores presentes na base de dados."]},{"cell_type":"code","metadata":{"id":"YP2O1C__RSna"},"source":["import numpy as np\n","from sklearn.neighbors import KNeighborsClassifier\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import load_iris\n","iris,cls = load_iris(return_X_y=True)\n","novaIris = pd.DataFrame({'area_sepala': iris[:,0] * iris[:,1],\n","                         'area_petala': iris[:,2] * iris[:,3]})\n","\n","print(novaIris.head())\n","\n","xMin, xMax = novaIris['area_sepala'].min() - 1, novaIris['area_sepala'].max() + 1\n","yMin, yMax = novaIris['area_petala'].min() - 1, novaIris['area_petala'].max() + 1\n","\n","xx, yy = np.meshgrid(np.arange(xMin - 0.1, xMax + 0.1, 0.1),\n","                     np.arange(yMin - 0.1, yMax + 0.1, 0.1))\n","\n","modelo = KNeighborsClassifier(n_neighbors = 7)\n","modelo.fit(novaIris, cls)\n","\n","ax = plt.gca()\n","Z = modelo.predict(np.c_[xx.ravel(), yy.ravel()])\n","Z = Z.reshape(xx.shape)\n","ax.contourf(xx, yy, Z, alpha=0.4)\n","ax.scatter(novaIris['area_sepala'], novaIris['area_petala'], c=cls, s=20, edgecolor='k')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zrdhxIQ8QsIm"},"source":["Validação cruzada\n","==================================\n","\n","O Scikit-Learn possui diversos utilitários para auxiliar na avaliação de modelos por meio de validação cruzada. Sugiro que você leia [esta página que os descreve](https://scikit-learn.org/stable/modules/cross_validation.html).\n","\n","Veremos dois usos comuns:\n","1. Obter o $E_{CV}$, ou seja, erro médio de validação cruzada;\n","2. Como ter o laço de controle da validação cruzada, tendo assim acesso à quais objetos estão em cada pasta.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"TlExsO3aHOrL"},"source":["## Obtendo $E_{CV}$\n","\n","Para isso podemos usar a função `cross_val_score`.\n","O método recebe quatro parâmetros:\n","- Um objeto com o classificador que deverá ser usado (ele será re-treinado a cada iteração da validação cruzada);\n","- A matriz de dados;\n","- O vetor com as saídas esperadas;\n","- O número de pastas que devem ser utilizadas (parâmetro `cv`).\n","\n","[Como pode ser visto aqui](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score), essa função tem diversos outros parâmetros que controlem inclusive o paralelismo de execução, mas estes sãos os básicos para o que queremos.\n","A função retorna o erro, por padrão usando a medida de avaliação *acurácia*, obtido em cada iteração.\n","\n"]},{"cell_type":"code","metadata":{"id":"KK5a2or6UHxW"},"source":["from sklearn.model_selection import cross_val_score\n","clf = KNeighborsClassifier(n_neighbors=1)\n","scores = cross_val_score(clf, novaIris, cls, cv=5)\n","print(scores)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UwJH_OaKYZUP"},"source":["## Laço de validação cruzada\n","\n","\n","Por vezes você vai precisar ter controle sobre os dados dentro do laço principal de validação cruzada, por exemplo, quando quer testar vários modelos e analisar os erros obtidos em cada pasta.\n","\n","A classe `StratifiedKFold` pode auxiliar nisso. Ela realiza a divisão dos objetos considerando a proporção dos objetos de cada classe. Em outras palavras, a mesma proporção de objetos em cada classe na base original é refletida em cada uma das pastas.\n","Seu uso é bem simples, funciona similar ao `StratifiedShuffleSplit` que vimos antes.\n","\n"]},{"cell_type":"code","metadata":{"id":"8Ff9NqKuGbuB"},"source":["from sklearn.model_selection import StratifiedKFold\n","\n","kf = StratifiedKFold(n_splits = 10)\n","for idx_treino, idx_teste in kf.split(novaIris, cls):\n","    Xtreino, ytreino = novaIris.iloc[idx_treino], cls[idx_treino]\n","    Xteste, yteste = novaIris.iloc[idx_teste], cls[idx_teste]\n","    erros = clf.fit(Xtreino, ytreino).predict(Xteste) != yteste\n","    print(f\"Erro de teste: {erros.sum()/len(idx_teste):.3f}\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YJARgmt1HXYJ"},"source":["# Seleção de modelos\n","\n","Vimos em aula que um procedimento muito utilizado para a seleção de modelos é a validação cruzada.\n","Basicamente, tínhamos que realizar validação cruzada para cada algoritmo (com sua configuração de hiperparâmetros) e escolher aquele que apresentava o menor $E_{CV}$.\n","\n","Implementar isso para pode ser um pouco tedioso, felizmente temos o `GridSearchCV` que nos permite fazer exatamente isso!\n","\n","A ideia consiste em termos um *grid* de parâmetros contendo todos os parâmetros que queremos variar para um algoritmo. O `GridSearchcV` vai simplesmente rodar todos (busca exaustiva) e selecionar o melhor.\n","Após selecionar o melhor, ele automaticamente (configurável via parâmetro), re-treina o modelo com todos os dados, **como deve ser feito**.\n","\n","[Vale a pena conhecer um pouco mais sobre essa funcionalidade no scikit-learn](https://scikit-learn.org/stable/modules/grid_search.html).\n","\n","[Caso queira usar o `GridSearchCV` para diferentes classificadores, vale a pena você conhecer sobre o `Pipeline`](https://scikit-learn.org/stable/modules/compose.html#pipeline)."]},{"cell_type":"code","metadata":{"id":"QuZXm2njLmXg"},"source":["from sklearn.model_selection import GridSearchCV\n","from pprint import PrettyPrinter\n","pp = PrettyPrinter()\n","\n","prm_grid = [ #cada elemento dessa lista é um dicionário com os parâmetros que devem ser buscados em exaustão e seus limites\n","    {'n_neighbors': [1, 3, 5, 7]}\n","]\n","knn = KNeighborsClassifier()\n","#validação cruzada de 5 pastas\n","cv = GridSearchCV(knn, prm_grid, cv = 5)\n","cv.fit(novaIris, cls)\n","print(\"Resultados encontrados na validação cruzada\")\n","pp.pprint(cv.cv_results_)\n","\n","print(\"Melhor parâmetro identificado\")\n","print(cv.best_params_)\n","print(\"Classificador re-treinado\")\n","print(cv.best_estimator_)\n"],"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1xZBOiD37lHmnCEalxzfG_VMqj3JWlp5v","timestamp":1740419213720}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}